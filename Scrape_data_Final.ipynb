{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrape_data_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wWYO0DjFJs1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "86b28c48-8705-4b8b-d9ec-bba2b2fd2961"
      },
      "source": [
        "!apt-get update"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rHit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [3 InRelease 2,586 B/88.7 kB 3%] [Waiting for headers]\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 2,586 B/88.7 k\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [3 InRelease 14.2 kB/88.7 kB 16%] [Waiting for he\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [875 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [908 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,205 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [844 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,376 kB]\n",
            "Fetched 5,476 kB in 2s (2,244 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c4JEX3_hztT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e2671699-5463-4b88-fa68-d8065dd5caac"
      },
      "source": [
        "!apt install chromium-chromedriver"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (81.0.4044.122-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO2koLfzh-h-",
        "colab_type": "code",
        "outputId": "e3c404aa-72a6-49fe-f68c-258151a7add8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y1rggyViPdG",
        "colab_type": "code",
        "outputId": "f44a42f3-0ebe-43cc-dcb5-410a14690b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install -U selenium"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSpQVUkvU6qb",
        "colab_type": "code",
        "outputId": "d5180a8d-98d2-40ca-b3ee-c934aaed5e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install lxml"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (4.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLnaSVUb_EHd",
        "colab_type": "text"
      },
      "source": [
        "# ***About Glassdoor.com*** \n",
        "![alt text](https://www.glassdoor.com/app/static/img/partners/fb/logo-1200x630.png?v=721f9cswl)\n",
        "\n",
        "   Glassdoor is a website where current and former employees anonymously review companies. Glassdoor also allows users to anonymously submit and view salaries as well as search and apply for jobs on its platform.\n",
        "\n",
        "   Glassdoor produces reports based upon the data its anonymous and, in many cases, managers generate positive reviews for their organizations through posts. These reports have been on topics including work-life balance, CEO pay-ratios, lists of the best office places and cultures,and the accuracy of corporate job searching maxims. Data from Glassdoor has also been used by outside sources to produce estimates on the effects of salary trends and changes on corporate revenues.Glassdoor also puts the conclusions of its research of other companies towards its own company policies. In 2015 Tom Lakin produced the first study of Glassdoor in the United Kingdom, concluding that Glassdoor is regarded by users as more trustworthy sources of information than career guides or official company documents.\n",
        "\n",
        "\n",
        "\n",
        "#  ***Logic of the scraping*** \n",
        "\n",
        "\n",
        "\n",
        "1.   In the first step extract links of every job listed from all the pages of the website.\n",
        "2.   Then we use selenium and beautiful soup(BS4) to to extract the page source.\n",
        "* Page source has a JSON which contains everything except Rating. \n",
        "\n",
        "* We use bs4 to extract rating from the mentioned tag and use class ='desc' to extract job descriptions. The reason for using 'desc' class is that it contains only text field and text has no extraneous HTML tags.\n",
        "3. A fair bit of warning don't put too many requests on the server as it may block your IP. Run this code wisely.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB19G5HHiRIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing libraries\n",
        "from selenium import webdriver\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "import time\n",
        "from selenium.common.exceptions import NoSuchElementException , ElementClickInterceptedException\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "import lxml\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n97sfT7xjYC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CheckUnique(links):\n",
        "\n",
        "    \n",
        "        count_dict = {link:links.count(link) for link in links if links.count(link) > 1}\n",
        "        clean_links = [link for link in links if link not in list(count_dict.keys())]\n",
        "        return clean_links\n",
        "\n",
        "def ScrapeLinks(driver , start_pg , end_pg ,total_pages):\n",
        "\n",
        "        #Total pages are the total web pages on the website not the total you want to scrape\n",
        "        if end_pg > total_pages:\n",
        "            print(\"Enter valid Page Indexes\")\n",
        "        br = driver\n",
        "        scr_links = []\n",
        "        url_head = 'https://www.glassdoor.co.in'\n",
        "        #br = self.Initialize_Driver()\n",
        "        for pg_num in range(start_pg, end_pg+1):\n",
        "            url = 'https://www.glassdoor.co.in/Job/new-york-data-scientist-jobs-SRCH_IL.0,8_IC1132348_KO9,23_IP'+str(i)+'.htm?fromAge=14'\n",
        "            br.get(url)\n",
        "            br.implicitly_wait(5)\n",
        "\n",
        "            soup = BeautifulSoup(br.page_source , 'lxml')\n",
        "\n",
        "            #print(f\"Scraping Page Number : {pg_num}\")\n",
        "            for a_tag in soup.find_all('a' , href = True):\n",
        "                pattern = re.compile('/partner/jobListing')\n",
        "                if pattern.match(str(a_tag['href'])):\n",
        "                    scr_links.append(url_head + str(a_tag['href']))\n",
        "        \n",
        "        op_links = CheckUnique(scr_links)\n",
        "        if len(op_links) == ((end_pg-start_pg) +1)*30 :\n",
        "            print(\"Scraping of links is completed\")\n",
        "            return op_links\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "def ScrapeData(num_jobs,driver , links):\n",
        "\n",
        "        br = driver\n",
        "        #data_df = df\n",
        "        #cleaned_links = ScrapeLinks(br)\n",
        "        #if data_df is None:\n",
        "        data_df = pd.DataFrame(columns = ['Job_title' , 'Company' , 'State' , 'City' , 'Min_Salary' ,'Max_Salary' ,'Job_Desc' , \n",
        "                                         'Industry'  , 'Rating' , 'Date_Posted' , 'Valid_until' , 'Job_Type' ,\n",
        "                                          ])\n",
        "        \n",
        "        start = time.time()\n",
        "        print(\"Gathering Information\")\n",
        "        for job in range(num_jobs):\n",
        "\n",
        "            #print(f'Extracting information about job : {job}')\n",
        "            br.get(links[job])\n",
        "            br.implicitly_wait(10)\n",
        "\n",
        "            soup = BeautifulSoup(br.page_source , 'lxml')\n",
        "\n",
        "            try:\n",
        "                \n",
        "                #The rest of the data is available in scrip = 'application/ld+json' i.e in  json file\n",
        "                json_file = soup.find('script' , {'type':'application/ld+json'})\n",
        "                op_json = json.loads(str(json_file.text) , strict = False)\n",
        "                \n",
        "                #title\n",
        "                try:\n",
        "                    title = op_json['title']\n",
        "                    #print(title)\n",
        "                except:\n",
        "                    title  = None\n",
        "                \n",
        "                #Dateposted\n",
        "                try:\n",
        "                    post_date = op_json['datePosted']\n",
        "                except:\n",
        "                    post_date = None\n",
        "                \n",
        "                #type\n",
        "                try:\n",
        "                    job_type = op_json['employmentType']\n",
        "                except:\n",
        "                    job_type = None\n",
        "\n",
        "                #Annual Salary Range\n",
        "                try:\n",
        "                    min_salary = int(op_json['estimatedSalary']['value']['minValue'])\n",
        "                    max_salary = int(op_json['estimatedSalary']['value']['maxValue'])\n",
        "                except:\n",
        "                    min_salary = -1\n",
        "                    max_salary = -1\n",
        "                \n",
        "                #Job Posting validity date(Y-M-D)\n",
        "                try:\n",
        "                    valid_date = op_json['validThrough']\n",
        "                except:\n",
        "                    valid_date = None\n",
        "                \n",
        "                #Industry\n",
        "                try:\n",
        "                    industry = op_json['industry']\n",
        "                except:\n",
        "                    industry = None\n",
        "                \n",
        "                #Location\n",
        "                try:\n",
        "                    city = op_json['jobLocation']['address']['addressLocality']\n",
        "                    state = op_json['jobLocation']['address']['addressRegion']\n",
        "                except:\n",
        "                    city , state = None , None\n",
        "\n",
        "                #Company\n",
        "                try:\n",
        "                    company = op_json['hiringOrganization']['name']\n",
        "                except:\n",
        "                    company = None\n",
        "                \n",
        "                #Let's get Description\n",
        "                try:\n",
        "                    desc = soup.find(class_ = 'desc').text\n",
        "                except:\n",
        "                    desc = None\n",
        "                try:\n",
        "                    rating = soup.find('span' , {'class' : 'css-1pmc6te e11nt52q3'}).text.replace('â˜…' , '')\n",
        "                \n",
        "                except:\n",
        "                    rating = None\n",
        "\n",
        "\n",
        "            \n",
        "                data_df = data_df.append({'Job_title' : title,\n",
        "                                      'Company' : company,\n",
        "                                      'State' : state,\n",
        "                                      'City' : city,\n",
        "                                      'Min_Salary':min_salary,\n",
        "                                      'Max_Salary':max_salary,\n",
        "                                      'Job_Desc' : desc,\n",
        "                                      'Industry':industry,\n",
        "                                      'Rating':rating,\n",
        "                                      'Date_Posted' : post_date,\n",
        "                                      'Valid_until' : valid_date,\n",
        "                                      'Job_Type' :job_type} , ignore_index = True)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        #driver.close()\n",
        "        print(f\"Scraping Completed for {data_df.shape[0]} jobs\")\n",
        "        print(f\"Time Required : {time.time() - start} seconds\")\n",
        "        \n",
        "        return data_df\n",
        "        \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-KPS5y2JlH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "244b530a-8494-457d-d4bf-fb0e8ba636b1"
      },
      "source": [
        "#Please Specify number of jobs as an excessive number of requests made by the computer Ip can potentially block you\n",
        "#glassdoor has 30 listing per page, so you can calculate manually how much is the total amount of jobs present i.e (num_pages*30)\n",
        "num_jobs = 150\n",
        "start_pg =1 \n",
        "end_pg = 30\n",
        "total_pages = 30\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage') \n",
        "driver = webdriver.Chrome('chromedriver' , options = options)\n",
        "\n",
        "links = ScrapeLinks(driver,start_pg,end_pg , 30)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scraping of links is completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3BoaNm0zvd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "58188b99-5e32-45c4-f612-be8960f1b5e7"
      },
      "source": [
        "links[:5]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.glassdoor.co.in/partner/jobListing.htm?pos=0&ao=37049&s=58&guid=&src=GD_JOB_AD&vt=w&cs=1i_7f4cef43&cb=1588782263572&jobListingId=3353360125',\n",
              " 'https://www.glassdoor.co.in/partner/jobListing.htm?pos=0&ao=8095&s=58&guid=&src=GD_JOB_AD&vt=w&cs=1i_1fb5cf82&cb=1588782263572&jobListingId=3331413851',\n",
              " 'https://www.glassdoor.co.in/partner/jobListing.htm?pos=0&ao=8095&s=58&guid=&src=GD_JOB_AD&vt=w&cs=1i_77680cf0&cb=1588782263572&jobListingId=3560810903',\n",
              " 'https://www.glassdoor.co.in/partner/jobListing.htm?pos=0&ao=4120&s=58&guid=&src=GD_JOB_AD&vt=w&cs=1i_a2726c63&cb=1588782263572&jobListingId=3452858287',\n",
              " 'https://www.glassdoor.co.in/partner/jobListing.htm?pos=0&ao=154438&s=58&guid=&src=GD_JOB_AD&vt=w&ea=1&cs=1i_8368af9d&cb=1588782263572&jobListingId=2836002558']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHGutLHDTk8h",
        "colab_type": "code",
        "outputId": "582c6b09-1684-4ff1-ab04-da733fb037e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "total_jobs = 30*(end_pg - start_pg+1)\n",
        "f_df = pd.DataFrame(columns = ['Job_title' , 'Company' , 'State' , 'City' , 'Min_Salary' ,'Max_Salary' ,'Job_Desc' , \n",
        "                                         'Industry'  , 'Rating' , 'Date_Posted' , 'Valid_until' , 'Job_Type' ,\n",
        "                                          ])\n",
        "dummy_df  = ScrapeData(10 , driver , links)\n",
        "#Exctract 150 jobs every 1 mins \n",
        "#for i in range(total_jobs//num_jobs):\n",
        "     #df = ScrapeData(num_jobs , driver , links[num_jobs*i : num_jobs*(i+1)])\n",
        "     #f_df = f_df.append(df)\n",
        "     #time.sleep(30)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gathering Information\n",
            "Scraping Completed for 10 jobs\n",
            "Time Required : 40.4912006855011 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t4to44SJM6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the file to csv\n",
        "f_df.to_csv('Data_Job_NY' , index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lq2_BsVvyfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SF_link  ='https://www.glassdoor.co.in/Job/san-francisco-data-scientist-jobs-SRCH_IL.0,13_IC1147401_KO14,28_IP'+str(pg_num)+'.htm?fromAge=14'\n",
        "NY_Link = 'https://www.glassdoor.co.in/Job/new-york-data-scientist-jobs-SRCH_IL.0,8_IC1132348_KO9,23.htm?fromAge=14'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
